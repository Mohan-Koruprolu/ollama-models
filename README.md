# ollama-models

Ever thought of running a LLM model locally ?

Yes, its possible to run Large Language models locally and the best part is they absolutely don't require internet connection once they are up and running. 

while there are many methods to do so , i want to share about Ollama 
https://ollama.com/
(Available for Mac, Linux ,windows preview)

->once installed visit their website for models 
 https://ollama.com/library

->select any model u want and copy the command
 eg :- ollama run wizard-math:7b 
 (7b indicates 7billion parameters)
that's it your model will be downloaded and after that even if you have no internet you could use the models 

now potential applications include 
1.) R.A.G (Retrieval.Augmented.Generation)
 simply stating It ensures that the model has access to the most current, reliable facts, and that users have access to the modelâ€™s sources, ensuring that its claims can be checked for accuracy and ultimately trusted
 
read more : https://lnkd.in/gw6yE_Ek

 2.) Private AI:- where companies or agencies could run their own models or implement R.A.G so that their data and security can be ensured
 
 3.) Model File:- changing model file can help in specifying the usecase to LLM in general than to mention it every time eg:- if we change the model file to socratic version the LLM would only give relevent       
      questions so that u could find the answer yourself 
 
